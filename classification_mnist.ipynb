{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnsit:  {'DESCR': 'mldata.org dataset: mnist-original', 'COL_NAMES': ['label', 'data'], 'target': array([ 0.,  0.,  0., ...,  9.,  9.,  9.]), 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ..., \n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}\n",
      "The data shape:  (70000, 784)\n",
      "The lable shape:  (70000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Datasets loaded by Scikit-Learn generally have a similar dictionary structure including:\n",
    "- A DESCR key describing the dataset\n",
    "- A data key containing an array with one row per instance and one column per feature\n",
    "- A target key containing an array with the labels\n",
    "'''\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "print('mnsit: ', mnist)\n",
    "x, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print('The data shape: ', x.shape)\n",
    "print('The lable shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a peek at one digit from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+CGU+CGU+CGU+CGU+CGU+CHUf5Zt+b+OQHReAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65f8999ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lable of x[36000]:  5.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\tinline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# grab an instance’s feature vector, reshape it to a 28×28 array, and display it using Matplotlib\n",
    "some_digit = x[36000]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
    "            interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "# Print the label\n",
    "print('The lable of x[36000]: ', y[36000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done..\n"
     ]
    }
   ],
   "source": [
    "# Create the training and the test sets and lables \n",
    "x_train, x_test, y_train, y_test = x[:60000], x[60000:], y[:60000], y[60000:]\n",
    "\n",
    "'''\n",
    "Shuffling the training set; this will guarantee that all cross-validation folds will be similar (youdon’t \n",
    "want one fold to be missing some digits). Moreover, some learning algorithms are sensitive to the order of the\n",
    "training instances, and they perform poorly if they get many similar instances in a row.\n",
    "'''\n",
    "import numpy as np\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLet’s simplify the problem for now and only try to identify one digit — for example, the number 5. This\\n“5-detector” will be an example of a binary classifier, capable of distinguishing between just two classes,\\n5 and not-5. Let’s create the target vectors for this classification task\\n\\ny_train_5 = (y_train == 5) # True for all 5s, False for all other digits.\\ny_test_5 = (y_test == 5)\\n\\n# Using Scikit-Learn’s SGDClassifier class\\nfrom sklearn.linear_model import SGDClassifier\\nsgd_clf = SGDClassifier(random_state=42)\\nsgd_clf.fit(x_train, y_train_5)\\n\\n# Detecting images of the number 5\\nsgd_clf.predict([some_digit])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Let’s simplify the problem for now and only try to identify one digit — for example, the number 5. This\n",
    "“5-detector” will be an example of a binary classifier, capable of distinguishing between just two classes,\n",
    "5 and not-5. Let’s create the target vectors for this classification task\n",
    "\n",
    "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits.\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "# Using Scikit-Learn’s SGDClassifier class\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(x_train, y_train_5)\n",
    "\n",
    "# Detecting images of the number 5\n",
    "sgd_clf.predict([some_digit])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_train_5 = (y_train == 5) # True for all 5s, False for all other digits.\\ny_test_5 = (y_test == 5)\\n\\nfrom sklearn.linear_model import SGDClassifier\\nsgd_clf = SGDClassifier(random_state=42)\\n\\nfrom sklearn.model_selection import StratifiedKFold\\nfrom sklearn.base import clone\\nskfolds = StratifiedKFold(n_splits=3, random_state=42)\\nfor train_index, test_index in skfolds.split(x_train, y_train_5):\\n    clone_clf = clone(sgd_clf)\\n    x_train_folds = x_train[train_index]\\n    y_train_folds = (y_train_5[train_index])\\n    x_test_fold = x_train[test_index]\\n    y_test_fold = (y_train_5[test_index])\\n    \\n    clone_clf.fit(x_train_folds, y_train_folds)\\n    y_pred = clone_clf.predict(x_test_fold)\\n    n_correct = sum(y_pred == y_test_fold)\\n    print(n_correct / len(y_pred)) # prints 0.9502, 0.96565 and 0.96495\\n\\n# Using cross_val_score()function to evaluate the SGDClassifier model using K-fold cross-validation,with 3 folds\\nfrom sklearn.model_selection import cross_val_score\\ncross_val_score(sgd_clf, x_train, y_train_5, cv=3, scoring=\"accuracy\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Occasionally you will need more control over the cross-validation process than what cross_val_score() and \n",
    "similar functions provide. In these cases, you can implement cross-validation yourself; it is actually fairly\n",
    "straightforward. The following code does roughly the same thing as the preceding cross_val_score() code, \n",
    "and prints the same result.\n",
    "The StratifiedKFold class performs stratified sampling to produce folds that contain a representative ratio of \n",
    "each class. At each iteration the code creates a clone of the classifier, trains that clone on the training \n",
    "folds, and makes predictions on the test fold. Then it counts the number of correct predictions and outputs \n",
    "the ratio of correct predictions.\n",
    "'''\n",
    "'''\n",
    "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits.\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "for train_index, test_index in skfolds.split(x_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    x_train_folds = x_train[train_index]\n",
    "    y_train_folds = (y_train_5[train_index])\n",
    "    x_test_fold = x_train[test_index]\n",
    "    y_test_fold = (y_train_5[test_index])\n",
    "    \n",
    "    clone_clf.fit(x_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(x_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred)) # prints 0.9502, 0.96565 and 0.96495\n",
    "\n",
    "# Using cross_val_score()function to evaluate the SGDClassifier model using K-fold cross-validation,with 3 folds\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, x_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nJust like the cross_val_score() function, cross_val_predict() performs K-fold cross-validation,\\nbut instead of returning the evaluation scores, it returns the predictions made on each test fold. This means\\nthat you get a clean prediction for each instance in the training set (“clean” meaning that the prediction is\\nmade by a model that never saw the data during training).\\n\\n\\nfrom sklearn.model_selection import cross_val_predict\\ny_train_pred = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3)\\n\\nUsing the confusion_matrix() function. Just pass it the target classes ( y_train_5 ) and the predicted \\nclasses ( y_train_pred )\\n\\nfrom sklearn.metrics import confusion_matrix\\nconfusion_matrix(y_train_5, y_train_pred)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Just like the cross_val_score() function, cross_val_predict() performs K-fold cross-validation,\n",
    "but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means\n",
    "that you get a clean prediction for each instance in the training set (“clean” meaning that the prediction is\n",
    "made by a model that never saw the data during training).\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3)\n",
    "\n",
    "Using the confusion_matrix() function. Just pass it the target classes ( y_train_5 ) and the predicted \n",
    "classes ( y_train_pred )\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an\\nimage represents a 5, it is correct only 77% of the time. Moreover, it only detects 79% of the 5s.\\n\\nfrom sklearn.metrics import precision_score, recall_score\\nprecision_score(y_train_5, y_pred) # 0.76871350203503808\\nrecall_score(y_train_5, y_train_pred) # 0.79136690647482011\\n\\nIt is often convenient to combine precision and recall into a single metric called the F1 score, in particular\\nif you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall.\\nWhereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision are high.\\n\\nfrom sklearn.metrics import f1_score\\nf1_score(y_train_5, y_pred)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Now your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an\n",
    "image represents a 5, it is correct only 77% of the time. Moreover, it only detects 79% of the 5s.\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_train_5, y_pred) # 0.76871350203503808\n",
    "recall_score(y_train_5, y_train_pred) # 0.79136690647482011\n",
    "\n",
    "It is often convenient to combine precision and recall into a single metric called the F1 score, in particular\n",
    "if you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall.\n",
    "Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values.\n",
    "As a result, the classifier will only get a high F1 score if both recall and precision are high.\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision/Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores\\nthat it uses to make predictions. Instead of calling the classifier’s predict() method, you can call its\\ndecision_function() method, which returns a score for each instance, and then make predictions based\\non those scores using any threshold you want:\\ny_scores = sgd_clf.decision_function([some_digit])\\ny_scores # array([161855.74572176])\\nthreshold = 0\\ny_some_digit_pred = (y_scores > threshold) # array([True], dtype=bool)\\n \\nHow can you decide which threshold to use? For this you will first need to get the scores of all instances in\\nthe training set using the cross_val_predict() function again, but this time specifying that you want it to \\nreturn decision scores instead of predictions\\n\\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\\n                            method=\"decision_function\")\\n\\nNow with these scores you can compute precision and recall for all possible thresholds using the\\nprecision_recall_curve() function\\n\\nfrom sklearn.metrics import precision_recall_curve\\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\\n\\n# Plot precision and recall as functions of the threshold value using Matplotlib\\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\\n    plt.xlabel(\"Threshold\")\\n    plt.legend(loc=\"upper left\")\\n    plt.ylim([0, 1])\\n    \\nplot_precision_recall_vs_threshold(precisions,\\trecalls,\\tthresholds)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores\n",
    "that it uses to make predictions. Instead of calling the classifier’s predict() method, you can call its\n",
    "decision_function() method, which returns a score for each instance, and then make predictions based\n",
    "on those scores using any threshold you want:\n",
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "y_scores # array([161855.74572176])\n",
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold) # array([True], dtype=bool)\n",
    " \n",
    "How can you decide which threshold to use? For this you will first need to get the scores of all instances in\n",
    "the training set using the cross_val_predict() function again, but this time specifying that you want it to \n",
    "return decision scores instead of predictions\n",
    "\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                            method=\"decision_function\")\n",
    "\n",
    "Now with these scores you can compute precision and recall for all possible thresholds using the\n",
    "precision_recall_curve() function\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "# Plot precision and recall as functions of the threshold value using Matplotlib\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "plot_precision_recall_vs_threshold(precisions,\trecalls,\tthresholds)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe receiver operating characteristic (ROC) curve is another common tool used with binary classifiers.\\nIt is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC\\ncurve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the\\nratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true\\nnegative rate, which is the ratio of negative instances that are correctly classified as negative. The TNR\\nis also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1 – specificity.\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various threshold values, using the\\nroc_curve() function\\n\\nfrom sklearn.metrics import roc_curve\\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\\n\\n# Plot the FPR against the TPR using Matplotlib\\ndef plot_roc_curve(fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth=2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--')\\n    plt.axis([0, 1, 0, 1])\\n    plt.xlabel('False Positive Rate')\\n    plt.ylabel('True Positive Rate')\\n    \\nplot_roc_curve(fpr, tpr)\\nplt.show()\\n\\nOnce again there is a tradeoff: the\\thigher the recall (TPR), the more false positives (FPR) the classifier\\nproduces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays\\nas far away from that line as possible (toward the top-left corner). One way to compare classifiers is to \\nmeasure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely\\nrandom classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC\\n\\nfrom sklearn.metrics import roc_auc_score\\nroc_auc_score(y_train_5, y_scores)\\n\\nSince the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one\\nto use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care\\nmore about the false positives than the false negatives, and the ROC curve otherwise. For example, looking at \\nthe previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is\\nmostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve \\nmakes it clear that the classifier has room for improvement (the curve could becloser to the top-right corner)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers.\n",
    "It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC\n",
    "curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the\n",
    "ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true\n",
    "negative rate, which is the ratio of negative instances that are correctly classified as negative. The TNR\n",
    "is also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1 – specificity.\n",
    "To plot the ROC curve, you first need to compute the TPR and FPR for various threshold values, using the\n",
    "roc_curve() function\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "# Plot the FPR against the TPR using Matplotlib\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()\n",
    "\n",
    "Once again there is a tradeoff: the\thigher the recall (TPR), the more false positives (FPR) the classifier\n",
    "produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays\n",
    "as far away from that line as possible (toward the top-left corner). One way to compare classifiers is to \n",
    "measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely\n",
    "random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)\n",
    "\n",
    "Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one\n",
    "to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care\n",
    "more about the false positives than the false negatives, and the ROC curve otherwise. For example, looking at \n",
    "the previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is\n",
    "mostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve \n",
    "makes it clear that the classifier has room for improvement (the curve could becloser to the top-right corner)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6633284dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score for the Random Forest Classifier:  0.99208699034\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Let’s train a RandomForestClassifier and compare its ROC curve and ROC AUC score to the\n",
    "SGDClassifier. First, you need to get scores for each instance in the training set. But due to the way it works,\n",
    "the RandomForestClassifier class does not have a decision_function() method. Instead it has a predict_proba()\n",
    "method. Scikit-Learn classifiers generally have one or the other. The predict_proba() method returns an array\n",
    "containing a row per instance and a column per class, each containing the probability that the given instance \n",
    "belongs to the given class (e.g., 70% chance that the image represents a 5)\n",
    "'''\n",
    "\n",
    "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits.\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "# Plot the FPR against the TPR using Matplotlib\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, x_train, y_train_5, cv=3,\n",
    "                                    method=\"predict_proba\")\n",
    "\n",
    "'''\n",
    "to plot a ROC curve, you need scores, not probabilities. A simple solution is to use the positive class’s \n",
    "probability as the score\n",
    "'''\n",
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\n",
    "\n",
    "# Plot the ROC curve. It is useful to plot the first ROC curve as well to see how they compare\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# ROC AUC score\n",
    "print('ROC AUC score for the Random Forest Classifier: ', roc_auc_score(y_train_5, y_scores_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross_val_score accuracy without scaling:  [ 0.94531094  0.93909695  0.94059109]\n",
      "The cross_val_score accuracy with scaling:  [ 0.94521096  0.93934697  0.9406411 ]\n"
     ]
    }
   ],
   "source": [
    "# Using the cross_val_score() function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print('The cross_val_score accuracy without scaling: ', cross_val_score(forest_clf, x_train, y_train, \n",
    "                                                                        cv=3, scoring=\"accuracy\")) \n",
    "\n",
    "# Scaling the inputs increases the accuracy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float64))\n",
    "\n",
    "print('The cross_val_score accuracy with scaling: ', cross_val_score(forest_clf, x_train_scaled, \n",
    "                                                                     y_train, cv=3, scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix: \n",
      " [[5805    0   22    6    9   18   27    3   31    2]\n",
      " [   1 6632   34   21   10    7    4   12   11   10]\n",
      " [  48   20 5657   48   37    7   27   46   51   17]\n",
      " [  29   23  150 5616   14  106   13   47   95   38]\n",
      " [  21   18   28    9 5544   10   34   20   17  141]\n",
      " [  37   17   27  196   29 4951   54    9   58   43]\n",
      " [  56   17   20    5   22   59 5707    1   29    2]\n",
      " [  13   29   75   31   76    8    1 5917   23   92]\n",
      " [  45   54   88  145   56   90   36   19 5248   70]\n",
      " [  40   17   26   93  151   37   10   87   61 5427]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65f4a5e5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(forest_clf, x_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "print('The confusion matrix: \\n', conf_mx) \n",
    "\n",
    "# It’s more convenient to look at an image representation of the confusion matrix, using Matplotlib’s matshow()\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCB9IiPCBhP4Ht7LWb3Yg2w4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65ebfa3b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Let’s focus the plot on the errors. First, you need to divide each value in the confusion matrix by the\n",
    "number of images in the corresponding class, so you can compare error rates instead of absolute numberof \n",
    "errors (which would make abundant classes look unfairly bad)\n",
    "'''\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "\n",
    "# Let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction:  [[False  True]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis assumes that all labels are equally important, which may not be the case. In particular, if you have\\nmany more pictures of Alice than of Bob or Charlie, you may want to give more weight to the classifier’s\\nscore on pictures of Alice. One simple option is to give each label a weight equal to its support (i.e., the\\nnumber of instances with that target label). To do this, simply set average=\"weighted\" in the preceding code\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Until now each instance has always been assigned to just one class. In some cases you may want your classifier \n",
    "to output multiple classes for each instance. For example, consider a face-recognition classifier: what should \n",
    "it do if it recognizes several people on the same picture? Of course it should attach one label per person it \n",
    "recognizes. Say the classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then when \n",
    "it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning “Alice yes, Bob no, Charlie \n",
    "yes”). Such a classification system that outputs multiple binary labels is called a multilabel classification \n",
    "system.\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(x_train, y_multilabel)\n",
    "\n",
    "'''\n",
    "This code creates a y_multilabel array containing two target labels for each digit image: the first indicates \n",
    "whether or not the digit is large (7, 8, or 9) and the second indicates whether or not it is odd. The next \n",
    "lines create a KNeighborsClassifier instance (which supports multilabel classification, but not all classifiers \n",
    "do) and we train it using the multiple targets array. Now you can make a prediction, and notice that it \n",
    "outputs two labels\n",
    "'''\n",
    "print('The prediction: ', knn_clf.predict([some_digit])) \n",
    "\n",
    "'''\n",
    "The digit 5 is indeed not large ( False ) and odd ( True ). There are many ways to evaluate a multilabel \n",
    "classifier, and selecting the right metric really depends on your project. For example, one approach is to \n",
    "measure the F1 score for each individual label (or any other binary classifier metric discussed earlier), \n",
    "then simply compute the average score. This code computes the average F1 score across all labels\n",
    "'''\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, x_train, y_train, cv=3)\n",
    "print('The F1 score: ', f1_score(y_train, y_train_knn_pred, average=\"macro\"))\n",
    "\n",
    "'''\n",
    "This assumes that all labels are equally important, which may not be the case. In particular, if you have\n",
    "many more pictures of Alice than of Bob or Charlie, you may want to give more weight to the classifier’s\n",
    "score on pictures of Alice. One simple option is to give each label a weight equal to its support (i.e., the\n",
    "number of instances with that target label). To do this, simply set average=\"weighted\" in the preceding code\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multioutput Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABU1JREFUeJzt3a9vFVkYgOF7N8XV4GgIkCBQYAgOi0KQVKAQkJCQYEn6\nH+AQBEeCAYdC4lAoRBVcDQhAQiBgKrpikzWbOXT74xZ4n8d+nc4RfXPE6czMt7e3Z0DPX4e9AOBw\niB+ixA9R4oco8UOU+CFK/BAlfogSP0StLPl+/p0QDt58Jz9k54co8UOU+CFK/BAlfogSP0SJH6LE\nD1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK\n/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+i\nxA9R4oco8UOU+CFK/BAlfohaOewFwJ9oa2trOD9y5MiSVjLNzg9R4oco8UOU+CFK/BAlfogSP0Q5\n5497//79cL5YLIbzkydPDuenTp2anH3//n147dra2nB+79694Xxzc3Ny9vTp0+G16+vrw/mPHz+G\n8+vXrw/nd+7cmZx9+vRpeO1+sfNDlPghSvwQJX6IEj9EiR+ixA9R8+3t7WXeb6k3q5jP54e9hJwl\nd/N/7egPws4PUeKHKPFDlPghSvwQJX6IEj9EeZ7/N/Dw4cPDXsKkjY2N4fzcuXMHdu+LFy8O56dP\nnz6we/8J7PwQJX6IEj9EiR+ixA9R4oco8UOU5/l/AaP3y89ms9mFCxd2/bt/9p34lRX/6vEH8jw/\nME38ECV+iBI/RIkfosQPUc55luD169fD+V6O8maz2ezx48eTM0d5TLHzQ5T4IUr8ECV+iBI/RIkf\nosQPUR7pXYJHjx4N57du3Tqwe//in5LmYHikF5gmfogSP0SJH6LED1HihyjxQ5Rz/iWYz3d07Hog\n7t+/P5xfvnx5OD9z5sx+LoflcM4PTBM/RIkfosQPUeKHKPFDlPghyjn/L+DLly/D+fPnz4fza9eu\n7frei8ViOL958+ZwfvXq1eF89K6C1dXV4bXsmnN+YJr4IUr8ECV+iBI/RIkfosQPUc75GXry5Mlw\nfuPGjeF8fX19cvbs2bPdLImfc84PTBM/RIkfosQPUeKHKPFDlKM+ht68eTOcX7lyZTh/+/bt5Gxz\nc3N47fnz54dzJjnqA6aJH6LED1HihyjxQ5T4IUr8EOWcnz35+PHjcH78+PFdX7u2trarNeGcHxgQ\nP0SJH6LED1HihyjxQ5T4IWrlsBfA7+3Vq1fD+bFjxyZnzvEPl50fosQPUeKHKPFDlPghSvwQJX6I\ncs7P0OfPn4fzu3fvDue3b9/ez+Wwj+z8ECV+iBI/RIkfosQPUeKHKK/uZmg+39FboCct+e+Lf3h1\nNzBN/BAlfogSP0SJH6LED1HihyiP9C7B2bNnh/MHDx4M5+/evRvOT5w48X+X9K8XL17s+trZbDZ7\n+fLlnq7n8Nj5IUr8ECV+iBI/RIkfosQPUeKHKOf8S7BYLIbzS5cuLWkl/7WxsTGcf/v2bThfXV3d\nz+WwRHZ+iBI/RIkfosQPUeKHKPFDlPghyjn/Evzs3fVbW1vD+devX4fzDx8+TM6OHj06vHYv7wLg\n92bnhyjxQ5T4IUr8ECV+iBI/RIkfouZL/n66j7XDwZvv5Ifs/BAlfogSP0SJH6LED1HihyjxQ5T4\nIUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oeoZX+ie0evFAYOnp0fosQP\nUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8\nECV+iPob3byufrwOPwwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65ebf5c438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "multioutput-multiclass classification (or simply multioutput classification). It is simply a generalization \n",
    "of multilabel classification where each label can be multiclass (i.e., it can have more than two possible \n",
    "values). To illustrate this, let’s build a system that removes noise from images. It will take as input a noisy \n",
    "digit image, and it will (hopefully) output a clean digit image, represented as an array of pixel intensities, \n",
    "just like the MNIST images. Notice that the classifier’s output is multilabel (one label per pixel) and each \n",
    "label can have multiple values (pixel intensity ranges from 0 to 255).\n",
    "'''\n",
    "'''\n",
    "Let’s start by creating the training and test sets by taking the MNIST images and adding noise to their pixel\n",
    "intensities using NumPy’s randint() function. The target images will be the original images\n",
    "'''\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "noise_train = np.random.randint(0, 100, (len(x_train), 784))\n",
    "noise_test = np.random.randint(0, 100, (len(x_test), 784))\n",
    "x_train_mod = x_train + noise_train\n",
    "x_test_mod = x_test + noise_test\n",
    "y_train_mod = x_train\n",
    "y_test_mod = x_test\n",
    "\n",
    "# let’s train the classifier and make it clean this image\n",
    "some_index = 5500\n",
    "knn_clf.fit(x_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([x_test_mod[some_index]])\n",
    "plot_digit(clean_digit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
